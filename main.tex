\documentclass[12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{tcolorbox}

% Set up colors
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\definecolor{darkred}{rgb}{0.5, 0, 0}
\definecolor{codebackground}{rgb}{0.95, 0.95, 0.95}
\definecolor{codeborder}{rgb}{0.8, 0.8, 0.8}

% Code listing configuration
\lstset{
    language=bash,
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{codebackground},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showspaces=false,
    showstringspaces=false,
    columns=fullflexible,
    frame=single,
    frameround=tttt,
    rulecolor=\color{codeborder},
    keywordstyle=\color{darkblue}\bfseries,
    commentstyle=\color{darkgreen},
    stringstyle=\color{darkred},
    captionpos=b
}

% Custom boxes
\newtcolorbox{importantbox}[1][]{%
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=Important,#1
}

\newtcolorbox{tipbox}[1][]{%
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Tip,#1
}

\newtcolorbox{warningbox}[1][]{%
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=Warning,#1
}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\rhead{Kubernetes Lab}
\lhead{Container Orchestration}
\cfoot{\thepage}

\title{\Huge Introduction to Kubernetes \& Container Orchestration \\
       \Large Deploying, Scaling, and Industry Context}
\author{Student Lab Manual}
\date{Target Audience: Student Group}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section*{Lab Overview}

This lab introduces \textbf{Kubernetes (K8s)}, the industry standard for container orchestration, using Minikube for hands-on local experience. Students will deploy and scale a microservice, understand the declarative architecture, and learn how Kubernetes compares to proprietary cloud services like AWS EC2 Auto Scaling and Azure App Service.

\textbf{Learning Objectives:} Students will understand Kubernetes architecture, deploy applications declaratively, demonstrate self-healing capabilities, and articulate Kubernetes' advantages in the real world.

\subsection*{Prerequisites}

This lab assumes you have a containerized microservice (Spring Boot application) from previous labs with:
\begin{itemize}
    \item A working \texttt{Dockerfile}
    \item An HTTP endpoint that responds to requests (e.g., \texttt{GET /})
    \item \textbf{Important}: An endpoint that performs CPU-intensive work (e.g., \texttt{GET /compute}) so the HPA can trigger on CPU load. If your microservice only returns static responses, the load test will not generate sufficient CPU usage to trigger autoscaling.
\end{itemize}

If your previous microservice lacks a CPU-intensive endpoint, you can add a simple one:
\begin{lstlisting}[caption=Example CPU-heavy endpoint in Spring Boot]
@GetMapping("/compute")
public String computeHeavy() {
    long result = 0;
    for (int i = 0; i < 1_000_000_000; i++) {
        result += i % 7;
    }
    return "Computed: " + result;
}
\end{lstlisting}

\newpage

\part{Part 1: Conceptual Foundation}

\section{Kubernetes Architecture (High-Level Overview)}

Kubernetes operates on a \textbf{desired state model}. You declare what you want (e.g., ``I want 3 replicas of my web app running''), and Kubernetes continuously works to make the actual state match the desired state.

\subsection{Control Plane (Master Node)}

The \textbf{Control Plane} is the brain of the cluster. It manages the desired state, schedules applications, and performs auto-healing.

\subsubsection{API Server}

The \textbf{API Server} is the only component you interact with (via \texttt{kubectl}). It is the central management hub for all cluster operations.

\subsubsection{etcd}

The \textbf{etcd} database is the cluster's single source of truth. It stores the entire desired state of your cluster—all Deployments, Services, Pods, and configurations.

\begin{importantbox}
If etcd fails, the cluster loses its memory. In production, etcd is replicated across multiple Control Plane nodes for high availability. In Minikube, there is a single etcd instance.
\end{importantbox}

\subsection{Worker Nodes}

\textbf{Worker Nodes} are the machines (physical servers or VMs) that actually run your applications. Each Worker Node contains:

\subsubsection{Kubelet}

The \textbf{Kubelet} is an agent running on every Worker Node. It communicates with the API Server to receive the desired state and ensures Pods are running and healthy.

\subsubsection{Container Runtime}

The \textbf{Container Runtime} (e.g., Docker or containerd) is the software that pulls container images and runs them. The Kubelet uses it to manage container lifecycles.

\subsection{Pods}

A \textbf{Pod} is the smallest deployable unit in Kubernetes. Although a Pod can contain multiple containers, it typically contains one application container. Pods are ephemeral—they are created and destroyed as needed.

\begin{tipbox}
Think of a Pod as a lightweight wrapper around your Docker container. Multiple Pods run on each Worker Node, distributed by the scheduler.
\end{tipbox}

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{\textbf{Kubernetes Cluster Structure}} \\
\hline
\textbf{Control Plane} & \textbf{Worker Node 1} & \textbf{Worker Node 2} \\
\hline
API Server & Pod 1 & Pod 3 \\
Scheduler & Pod 2 & Pod 4 \\
etcd & Container Runtime & Container Runtime \\
\hline
\end{tabular}
\end{figure}

\section{How Autoscaling Works (HPA)}

The \textbf{Horizontal Pod Autoscaler (HPA)} is a Controller that constantly monitors metrics (like CPU usage) of a Deployment.

\subsection{The HPA Workflow}

\begin{enumerate}
    \item \textbf{Monitoring}: The HPA checks the average CPU utilization of all running Pods against a target you define (e.g., 50\%).

    \item \textbf{Calculation}: If the current average exceeds the target, the HPA calculates the ideal number of replicas needed:
    \[
    \text{desired\_replicas} = \left\lceil \frac{\text{current\_utilization}}{\text{target\_utilization}} \times \text{current\_replicas} \right\rceil
    \]

    \item \textbf{Action}: The HPA updates the \texttt{replicas} field in the Deployment object.

    \item \textbf{Self-Healing}: The Deployment Controller sees the updated replica count and automatically creates new Pods to match the desired state.
\end{enumerate}

\begin{importantbox}
The HPA doesn't directly create Pods. Instead, it updates the Deployment's desired replica count, and the Deployment Controller handles actual Pod creation. This is the power of Kubernetes' declarative model.
\end{importantbox}

\section{Kubernetes in the Real World}

In this section, we compare Kubernetes with proprietary cloud services to understand how Kubernetes fits into the industry landscape.

\subsection{Comparison Table}

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{|l|X|X|X|}
\toprule
\textbf{Feature} & \textbf{AWS EC2 Auto Scaling} & \textbf{Azure App Service} & \textbf{Kubernetes (EKS, AKS, GKE)} \\
\midrule
\textbf{Abstraction Level} &
Infrastructure (VMs). Scales the underlying virtual machines. &
Platform (Web Apps). Fully managed web apps/APIs. &
Container (Pods). Scales containerized workload within VMs. \\
\midrule
\textbf{Scaling Unit} &
EC2 Virtual Machines. Scaling takes minutes. &
App Service Plan Instances. Scaling is relatively fast. &
Pods. Scaling takes seconds. Extremely fast. \\
\midrule
\textbf{Vendor Lock-in} &
High. Tied deeply to AWS APIs. &
High. Tied deeply to Azure APIs. &
Low (Portable). YAML files work across AWS (EKS), Azure (AKS), GCP (GKE), and on-premise clusters. \\
\midrule
\textbf{Control \& Flexibility} &
High control over OS/VM layer. &
Low control; highly opinionated. &
High control over deployment, networking, and storage. \\
\midrule
\textbf{Primary Use Case} &
Scaling monolithic applications where VM is the boundary. &
Simple web applications and APIs with zero infrastructure overhead. &
Complex Microservices architectures requiring cross-cloud portability and advanced deployment strategies. \\
\bottomrule
\end{tabularx}
\caption{Kubernetes vs. Cloud Scaling Services}
\end{table}

\subsection{Key Insights}

\subsubsection{Pod vs. VM Scaling}

\textbf{AWS EC2 Auto Scaling} scales at the VM level—launching new EC2 instances takes minutes and is expensive. \textbf{Kubernetes} scales at the Pod level (which runs inside existing VMs)—spinning up a new Pod takes seconds and is much cheaper.

\subsubsection{Portability}

\textbf{Azure App Service} and \textbf{AWS EC2 Auto Scaling} are proprietary solutions tightly coupled to their cloud providers. Migration requires rewriting infrastructure.

\textbf{Kubernetes} is open-source. Your YAML files work identically across AWS (EKS), Azure (AKS), Google Cloud (GKE), and on-premise clusters. This avoids vendor lock-in.

\subsubsection{Complexity vs. Control}

\begin{itemize}
    \item \textbf{Azure App Service}: Simplest—upload your code, Azure manages everything. Limited control.
    \item \textbf{AWS EC2 Auto Scaling}: More control but operates at VM level (coarse-grained).
    \item \textbf{Kubernetes}: Fine-grained control over deployment, networking, and storage. Steeper learning curve.
\end{itemize}

\subsection{When to Use Kubernetes}

Kubernetes is the right choice when you need:

\begin{itemize}
    \item \textbf{Microservices Architecture}: Multiple independent services scaling at different rates.
    \item \textbf{Cross-Cloud Portability}: Avoid vendor lock-in and maintain deployment flexibility.
    \item \textbf{Fine-Grained Control}: Need control over networking, storage, and deployment strategies.
    \item \textbf{Fast Scaling}: Need to scale in seconds without waiting for VM provisioning.
\end{itemize}

\newpage

\part{Part 2: Hands-On Deployment}

\section{Step 1: Local Kubernetes Environment Setup}

\subsection{1.1: Install Minikube and kubectl}

\subsubsection{Prerequisites}

\begin{itemize}
    \item Docker Desktop (with Docker daemon running)
    \item Homebrew (macOS) or appropriate package manager
\end{itemize}

\begin{importantbox}
If you don't have Docker Desktop, download it from \url{https://www.docker.com/products/docker-desktop}.
\end{importantbox}

\subsubsection{macOS Installation}

\begin{lstlisting}[caption=Install Minikube and kubectl on macOS]
# Install Minikube using Homebrew
brew install minikube

# Install kubectl
brew install kubectl

# Verify installations
minikube version
kubectl version --client
\end{lstlisting}

\subsubsection{Linux Installation}

\begin{lstlisting}[caption=Install Minikube and kubectl on Linux]
# Download Minikube
curl -Lo minikube https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
chmod +x minikube
sudo mv minikube /usr/local/bin/

# Install kubectl
sudo apt-get update
sudo apt-get install -y kubectl

# Verify installations
minikube version
kubectl version --client
\end{lstlisting}

\subsubsection{Windows Installation}

\begin{lstlisting}[caption=Install Minikube and kubectl on Windows]
# Using Chocolatey
choco install minikube
choco install kubernetes-cli

# Or download from GitHub:
# https://github.com/kubernetes/minikube/releases
# https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/

minikube version
kubectl version --client
\end{lstlisting}

\subsection{1.2: Start the Local Kubernetes Cluster}

\begin{lstlisting}[caption=Start Minikube cluster]
# Start the Minikube cluster
minikube start

# This creates a local Kubernetes cluster with:
# - Control Plane components
# - One Worker Node
# - Networking and storage configured
# - kubectl configured to point to the cluster
\end{lstlisting}

\begin{tipbox}
First startup takes a few minutes. Subsequent starts are faster.
\end{tipbox}

\subsection{1.3: Connect to Minikube's Docker Daemon}

Minikube includes its own Docker daemon. To use it:

\begin{lstlisting}[caption=Connect to Minikube Docker]
# Evaluate environment variables to point to Minikube's Docker
eval $(minikube docker-env)

# Verify you're using Minikube's Docker
docker info | grep "Operating System"
# Output should show: minikube
\end{lstlisting}

\begin{importantbox}
This step is critical! If you skip it, Kubernetes will try to pull your image from Docker Hub and fail.
\end{importantbox}

\subsection{1.4: Build the Container Image}

Assume you have a Spring Boot microservice (from previous labs) with a Dockerfile.

\begin{lstlisting}[caption=Build Docker image]
# Make sure you're connected to Minikube's Docker
eval $(minikube docker-env)

# Build the image
docker build -t microservice:v1 .

# Verify the image was created
docker images | grep microservice
\end{lstlisting}

\subsection{1.5: Verify Cluster Status}

\begin{lstlisting}[caption=Verify cluster status]
# Check nodes in the cluster
kubectl get nodes

# Expected output:
# NAME       STATUS   ROLES           AGE    VERSION
# minikube   Ready    control-plane   2m     v1.xx.x

# Get detailed node information
kubectl describe node minikube
\end{lstlisting}

\begin{tipbox}
If status shows \texttt{NotReady}, wait for the cluster to fully initialize.
\end{tipbox}

\newpage

\section{Step 2: Deployment and Service}

\subsection{2.1: Create Deployment YAML}

A \textbf{Deployment} describes the desired state of your application: container image, number of replicas, port mappings, and resource limits.

Create \texttt{deployment.yaml}:

\begin{lstlisting}[caption=deployment.yaml - Kubernetes Deployment]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice-deployment
  labels:
    app: microservice
spec:
  # Initial number of replicas
  replicas: 1

  # Selector identifies Pods managed by this Deployment
  selector:
    matchLabels:
      app: microservice

  # Template for creating Pods
  template:
    metadata:
      labels:
        app: microservice
    spec:
      containers:
      - name: microservice
        image: microservice:v1
        imagePullPolicy: Never  # Don't pull from Docker Hub
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
\end{lstlisting}

\textbf{Key Configuration:}

\begin{description}
    \item[imagePullPolicy: Never] Use only locally-built images. Don't try to pull from Docker Hub.
    \item[replicas: 1] Start with 1 Pod. We'll scale this later.
    \item[resources.requests] Minimum CPU/memory the Pod needs.
    \item[resources.limits] Maximum CPU/memory the Pod can use.
\end{description}

\subsection{2.2: Create Service YAML}

A \textbf{Service} exposes your Deployment to the network, providing stable IP addresses and load balancing.

Create \texttt{service.yaml}:

\begin{lstlisting}[caption=service.yaml - Kubernetes Service]
apiVersion: v1
kind: Service
metadata:
  name: microservice-service
  labels:
    app: microservice
spec:
  type: NodePort

  # Selector matches Pods with this label
  selector:
    app: microservice

  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
    nodePort: 30080
    name: http
\end{lstlisting}

\textbf{Service Types:}

\begin{description}
    \item[ClusterIP] Internal only (default).
    \item[NodePort] Exposes on each Node's IP address.
    \item[LoadBalancer] Provisions external load balancer (cloud only).
\end{description}

For Minikube, we use \texttt{NodePort}.

\subsection{2.3: Apply Configuration}

\begin{lstlisting}[caption=Deploy to cluster]
# Apply the Deployment
kubectl apply -f deployment.yaml

# Apply the Service
kubectl apply -f service.yaml

# Verify Deployment was created
kubectl get deployments

# Verify Service was created
kubectl get services
\end{lstlisting}

\subsection{2.4: Verify Pod Creation}

\begin{lstlisting}[caption=Check Pods]
# List all Pods
kubectl get pods

# Get detailed Pod info
kubectl describe pod <pod-name>

# View Pod logs
kubectl logs <pod-name>
\end{lstlisting}

\subsection{2.5: Access the Service}

\begin{lstlisting}[caption=Access the service]
# Get the external URL
minikube service microservice-service --url

# Test the endpoint
curl <returned-url>/

# Or open in browser
minikube service microservice-service
\end{lstlisting}

\begin{tipbox}
If you see ``Connection refused'', wait a few seconds for the Pod to initialize.
\end{tipbox}

\newpage

\section{Step 3: Declarative Management and Self-Healing}

\subsection{3.1: Manual Scaling (Scale Up)}

Instead of manually creating Pods, declare the desired number:

\begin{lstlisting}[caption=Scale to 3 replicas]
# Scale the Deployment
kubectl scale deployment microservice-deployment --replicas=3

# Verify 3 Pods are running
kubectl get pods

# Expected output (after a few seconds):
# NAME                                    READY   STATUS    AGE
# microservice-deployment-abc123def456    1/1     Running   3m
# microservice-deployment-ghi789jkl012    1/1     Running   10s
# microservice-deployment-mno345pqr678    1/1     Running   10s
\end{lstlisting}

\begin{importantbox}
Kubernetes creates new Pods according to the desired state. The Service automatically includes these new Pods in load balancing.
\end{importantbox}

\subsection{3.2: Demonstrating Self-Healing}

Kubernetes' most powerful feature: automatic Pod replacement if one fails.

\begin{lstlisting}[caption=Pod self-healing]
# Get Pod name
kubectl get pods

# Delete a Pod
kubectl delete pod <pod-name>

# Check Pods immediately
kubectl get pods

# Within seconds:
# - Deleted Pod disappears
# - NEW Pod is created with different name
# - Total count returns to 3

# This is self-healing!
\end{lstlisting}

\begin{tipbox}
The Pod name changes (Kubernetes generates random suffixes), but the label (\texttt{app: microservice}) remains the same.
\end{tipbox}

\subsection{3.3: Scaling Down}

\begin{lstlisting}[caption=Scale down]
# Reduce to 1 replica
kubectl scale deployment microservice-deployment --replicas=1

# Observe Pods
kubectl get pods

# 2 Pods are terminated, 1 remains
\end{lstlisting}

\newpage

\section{Step 4: Horizontal Pod Autoscaler (HPA)}

\subsection{4.1: Enable Metrics Server}

The HPA uses metrics from the Metrics Server:

\begin{lstlisting}[caption=Enable Metrics Server]
# Enable the addon
minikube addons enable metrics-server

# Verify it's running
kubectl get deployment metrics-server -n kube-system

# Wait for it to be ready (30+ seconds)
kubectl get deployment metrics-server -n kube-system --watch
# Press Ctrl+C to stop watching
\end{lstlisting}

\begin{importantbox}
The Metrics Server collects CPU/memory usage from Pods. Without it, the HPA won't work.
\end{importantbox}

\subsection{4.2: Create the HPA}

\begin{lstlisting}[caption=Create HPA with kubectl]
# Create HPA for automatic scaling
kubectl autoscale deployment microservice-deployment \
  --cpu-percent=50 \
  --min=1 \
  --max=5

# This means:
# - Scale when CPU > 50% of requested CPU
# - Minimum 1 Pod
# - Maximum 5 Pods
\end{lstlisting}

\subsection{4.3: Verify Metrics Collection}

Before generating load, verify that metrics are being collected properly:

\begin{lstlisting}[caption=Verify metrics are flowing]
# Wait 30-60 seconds for metrics server to initialize
# Then check if metrics are available
kubectl top pods

# Expected output:
# NAME                                    CPU(cores)   MEMORY(bytes)
# microservice-deployment-abc123def456    50m          128Mi
#
# If you see no output or errors, wait longer and try again
\end{lstlisting}

\begin{importantbox}
The Metrics Server takes time to collect baseline metrics. You must see CPU values above before the HPA will function. If you see ``<unknown>'' in HPA status, this means metrics are still being collected.
\end{importantbox}

\subsection{4.4: Monitor HPA Status}

\begin{lstlisting}[caption=Monitor HPA]
# Check HPA status
kubectl get hpa

# Expected output (after metrics are available):
# NAME                          REFERENCE                              TARGETS        MINPODS  MAXPODS  REPLICAS
# microservice-deployment       Deployment/microservice-deployment     5%/50%         1        5        1

# Watch in real-time
kubectl get hpa --watch
# Press Ctrl+C to stop
\end{lstlisting}

\begin{tipbox}
Initially you may see \texttt{<unknown>/50\%}. This is normal—the Metrics Server collects data for 1-2 minutes before it can calculate averages. If this persists beyond 2 minutes, restart the metrics-server addon.
\end{tipbox}

\subsection{4.5: Generate Load}

Simulate CPU-intensive traffic to trigger scaling:

\begin{importantbox}
\textbf{Critical}: To trigger the HPA, you must hit the CPU-heavy endpoint (e.g., \texttt{/compute}) that you added to your microservice. Simply hitting a static response endpoint will not generate enough CPU load. If your original microservice doesn't have a CPU-intensive endpoint, the HPA will not scale.
\end{importantbox}

\begin{lstlisting}[caption=Generate load with curl loop on CPU-heavy endpoint]
# Terminal 1: Get service URL
SERVICE_URL=$(minikube service microservice-service --url)

# Terminal 2: Generate load with concurrent requests
# Use multiple parallel processes for more aggressive load
for i in {1..10}; do
  while true; do
    curl -s $SERVICE_URL/compute > /dev/null
  done &
done

# Terminal 3: Watch HPA scale up in real-time
kubectl get hpa --watch

# You should see:
# REPLICAS increasing: 1 -> 2 -> 3 -> 4 -> 5 (may take 1-3 minutes)
# TARGETS showing CPU % rising above 50%

# If replicas don't increase after 5 minutes, try more parallel requests:
# for i in {1..20}; do ... (increase from 10 to 20)
\end{lstlisting}

\begin{tipbox}
\textbf{Alternative}: If you want stronger load, install \texttt{apache2-utils} and use \texttt{ab} (Apache Bench):
\begin{verbatim}
ab -n 100000 -c 50 $SERVICE_URL/compute
\end{verbatim}
This creates 50 concurrent connections and sends 100,000 requests, generating much more CPU load than simple curl loops.
\end{tipbox}

\subsection{4.6: Observe Scaling Down}

\begin{lstlisting}[caption=Stop load and watch scale-down]
# In Terminal 2, press Ctrl+C to stop load generation

# In Terminal 3, continue watching
kubectl get hpa --watch

# After the stabilization period (described below):
# REPLICAS decreases gradually back toward 1
# TARGETS shows low CPU usage
\end{lstlisting}

\begin{importantbox}
\textbf{HPA Timing Behavior}:
\begin{itemize}
    \item \textbf{Scale-up}: HPA checks metrics every 30 seconds. When CPU exceeds 50\%, it scales up immediately (if there's capacity).
    \item \textbf{Scale-down}: HPA has a 5-minute stabilization period after the last scale-up before it scales down. This prevents rapid oscillations when load fluctuates.
    \item \textbf{Metric delay}: Metrics are polled every 30 seconds, so you may see a 30-second delay between stopping load and seeing CPU usage drop.
\end{itemize}
\textbf{Example timeline}: Stop load at T=0 → CPU drops at T=30s → HPA begins scale-down at T=5m30s.
\end{importantbox}

\section{Step 5: Clean Up}

After the lab, free up resources:

\begin{lstlisting}[caption=Clean up resources]
# Delete the Service
kubectl delete service microservice-service

# Delete the Deployment (also deletes Pods)
kubectl delete deployment microservice-deployment

# Delete the HPA
kubectl delete hpa microservice-deployment

# Verify everything is deleted
kubectl get deployments
kubectl get services
kubectl get pods

# Stop Minikube (optional)
minikube stop

# Delete Minikube cluster entirely (optional)
minikube delete
\end{lstlisting}

\newpage

\part{Part 3: Lab Demonstration and Evaluation}

\section{What Students Must Demonstrate}

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{|l|X|X|}
\toprule
\textbf{Component} & \textbf{Evidence Required} & \textbf{Focus Area} \\
\midrule
\textbf{K8s Core Objects} &
Show running Pods with \texttt{kubectl get pods}. Access service URL successfully. &
Deployment and networking configuration. \\
\midrule
\textbf{Self-Healing} &
Delete a Pod with \texttt{kubectl delete pod <pod-name>}. Show new Pod automatically created. Replica count unchanged. &
Declarative state and Deployment Controllers. \\
\midrule
\textbf{Horizontal Autoscaling} &
Show HPA creation. Display \texttt{kubectl get hpa} output. Demonstrate replica count increasing under simulated load. &
Orchestration power and automated scaling. \\
\midrule
\textbf{Conceptual Understanding} &
Verbally explain: How HPA differs from AWS EC2 Auto Scaling (Container vs. VM scaling). Why Kubernetes is portable. &
Real-world context and industry relevance. \\
\bottomrule
\end{tabularx}
\end{table}

\section{Sample Interview Questions}

\begin{enumerate}
    \item \textbf{Q: Explain the Kubernetes desired state model.}

    \textit{A:} You declare what you want (desired state) in YAML files. Kubernetes continuously monitors the actual state and makes changes to match the desired state. If a Pod fails, Kubernetes replaces it automatically.

    \item \textbf{Q: What is the difference between the Control Plane and Worker Nodes?}

    \textit{A:} The Control Plane manages the cluster (scheduling, desired state, health). Worker Nodes run the Pods. The Control Plane decides where Pods go; Worker Nodes execute those decisions.

    \item \textbf{Q: Why is Kubernetes faster to scale than AWS EC2?}

    \textit{A:} Kubernetes scales Pods in seconds (they're lightweight containers already inside VMs). EC2 takes minutes because it must provision entire new virtual machines, including OS boot time.

    \item \textbf{Q: What does ``imagePullPolicy: Never'' do and why did we use it?}

    \textit{A:} It tells Kubernetes to only use locally-built images, not pull from Docker Hub. We used it because our image doesn't exist in Docker Hub—it's only in Minikube's local Docker daemon.

    \item \textbf{Q: How is Kubernetes more portable than AWS services?}

    \textit{A:} Kubernetes is cloud-agnostic. The same YAML files work on AWS (EKS), Azure (AKS), Google Cloud (GKE), and on-premise. AWS services are proprietary and tied to their APIs.

    \item \textbf{Q: What does the HPA do when CPU usage drops below 50\%?}

    \textit{A:} After a 5-minute stabilization period, the HPA scales down by reducing replicas. This prevents thrashing (rapid scale up/down) when load fluctuates.
\end{enumerate}

\section{Expected Outcomes Checklist}

By the end of this lab, students should be able to:

\begin{itemize}
    \item[$\square$] Set up and interact with a local Kubernetes cluster using Minikube and kubectl
    \item[$\square$] Deploy an application using Kubernetes Deployment and expose it using a Service
    \item[$\square$] Demonstrate the declarative, self-healing, and scaling principles of Kubernetes
    \item[$\square$] Articulate the high-level K8s architecture (Control Plane vs. Worker Node)
    \item[$\square$] Compare and contrast Kubernetes with proprietary cloud scaling solutions
    \item[$\square$] Explain why Kubernetes is portable and avoids vendor lock-in
    \item[$\square$] Manually scale a Deployment and observe automatic Pod creation
    \item[$\square$] Demonstrate Kubernetes self-healing by deleting a Pod and observing replacement
    \item[$\square$] Create and monitor a Horizontal Pod Autoscaler
    \item[$\square$] Explain the difference between Pod-level scaling and VM-level scaling
\end{itemize}

\newpage

\appendix

\section{Troubleshooting Guide}

\subsection{Minikube Issues}

\subsubsection{Error: Minikube fails to start}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Ensure Docker Desktop is running
    \item Check available disk space: \texttt{df -h}
    \item Delete old cluster: \texttt{minikube delete}
    \item Try again: \texttt{minikube start}
\end{enumerate}
\end{warningbox}

\subsubsection{Error: kubectl cannot connect}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Verify Minikube is running: \texttt{minikube status}
    \item Reset kubectl: \texttt{kubectl config use-context minikube}
    \item Check kubectl is installed: \texttt{kubectl version --client}
\end{enumerate}
\end{warningbox}

\subsection{Docker Issues}

\subsubsection{Error: Docker image not found when deploying}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Verify you ran: \texttt{eval \$(minikube docker-env)}
    \item Verify image exists: \texttt{docker images | grep microservice}
    \item Rebuild image: \texttt{docker build -t microservice:v1 .}
    \item Delete failed Pod: \texttt{kubectl delete pod <pod-name>}
    \item Kubernetes creates new Pod with correct image
\end{enumerate}
\end{warningbox}

\subsection{Pod Issues}

\subsubsection{Error: Pod is in CrashLoopBackOff}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Check logs: \texttt{kubectl logs <pod-name>}
    \item Check resources: \texttt{kubectl describe pod <pod-name>}
    \item Resource limits may be too low—increase in deployment.yaml
    \item Redeploy: \texttt{kubectl apply -f deployment.yaml}
\end{enumerate}
\end{warningbox}

\subsubsection{Error: Service URL returns connection refused}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Wait for Pod to initialize (30 seconds)
    \item Check Pod status: \texttt{kubectl get pods}
    \item Verify Service points to correct port: \texttt{kubectl get service}
    \item Check logs: \texttt{kubectl logs <pod-name>}
\end{enumerate}
\end{warningbox}

\subsection{HPA Issues}

\subsubsection{Error: HPA shows \texttt{<unknown>/50\%}}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Metrics Server is still collecting data—wait 1-2 minutes
    \item Verify running: \texttt{kubectl get deployment metrics-server -n kube-system}
    \item Check metrics: \texttt{kubectl top pods}
    \item Restart if needed: disable and re-enable addon
\end{enumerate}
\end{warningbox}

\subsubsection{Error: Pods not scaling up under load}

\begin{warningbox}
\textbf{Solution:}
\begin{enumerate}
    \item Verify HPA exists: \texttt{kubectl get hpa}
    \item Verify Deployment has resource requests (HPA uses these)
    \item Check HPA status: \texttt{kubectl describe hpa <hpa-name>}
    \item Generate more aggressive load
    \item Check events: \texttt{kubectl describe hpa <hpa-name>}
\end{enumerate}
\end{warningbox}

\section{Quick Reference: kubectl Commands}

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{|l|X|}
\toprule
\textbf{Command} & \textbf{Description} \\
\midrule
\texttt{kubectl get nodes} & List cluster nodes \\
\texttt{kubectl get pods} & List Pods \\
\texttt{kubectl get deployments} & List Deployments \\
\texttt{kubectl get services} & List Services \\
\texttt{kubectl get hpa} & List Horizontal Pod Autoscalers \\
\texttt{kubectl describe pod <name>} & Detailed Pod information \\
\texttt{kubectl logs <pod-name>} & View Pod logs \\
\texttt{kubectl delete pod <name>} & Delete a Pod \\
\texttt{kubectl scale deployment <name> --replicas=N} & Scale Deployment \\
\texttt{kubectl apply -f <file.yaml>} & Apply YAML configuration \\
\texttt{kubectl edit deployment <name>} & Edit Deployment \\
\bottomrule
\end{tabularx}
\end{table}

\section{Key Kubernetes Concepts}

\begin{description}
    \item[Desired State] What you declare in YAML (e.g., 3 replicas running)
    \item[Actual State] What's currently running in the cluster
    \item[Reconciliation] Kubernetes continuously adjusting actual state to match desired state
    \item[Declarative] You say WHAT you want, Kubernetes figures out HOW to achieve it
    \item[Imperative] You say HOW to do something (traditional scripts)
    \item[Controller] A Kubernetes component that watches desired state and makes corrections
    \item[Replica] A copy of your Pod running independently
    \item[Scale] Increase/decrease number of replicas
    \item[Self-Healing] Automatic replacement of failed Pods
\end{description}

\end{document}
